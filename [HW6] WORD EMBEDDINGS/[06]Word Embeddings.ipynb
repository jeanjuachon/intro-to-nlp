{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework, I have used the pre-trained word embeddings by Github user @Kyubyong. <br>The word embeddings were trained via word2vec and fasttext. The vector size of the word embeddings is 100 based on a corpus size of 38m and vocabulary size of 10,068.<br>\n",
    "I have used the gensim library for loading the word embeddings rather than using the original fasttext for consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task One: Top 10 most similar of 5 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/philip/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "wv_model = Word2Vec.load(\"/Users/philip/Desktop/tagalog-word-embeddings/tl.bin\") #Load the pretrained word2vec\n",
    "ft_model = FastText.load_fasttext_format(\"/Users/philip/Desktop/tagalog-word-embeddings/tlf.bin\") #Load the fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simGen(string):\n",
    "    wv_results = wv_model.wv.most_similar(string)\n",
    "    ft_results = ft_model.wv.most_similar(string)\n",
    "    wvDF = pd.DataFrame(wv_results,columns=[\"Word\",\"%Similarity\"]).rename(index = lambda x: x+1)\n",
    "    wvDF['%Similarity'] = round(wvDF['%Similarity'] * 100, 2)\n",
    "    ftDF = pd.DataFrame(ft_results,columns=[\"Word\",\"%Similarity\"]).rename(index = lambda x: x+1)\n",
    "    ftDF['%Similarity'] = round(ftDF['%Similarity'] * 100, 2)\n",
    "    print(f\"Word2Vec Results: \\n{wvDF}\")\n",
    "    #print(f\"Similarities w2v: \\n{wvDF['%Similarity']}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"FastText Results: \\n{ftDF}\")\n",
    "    #print(f\"Similarities fastText: \\n{ftDF['%Similarity']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "                Word  %Similarity\n",
      "1          pangulong        75.88\n",
      "2          ferdinand        72.92\n",
      "3            estrada        72.53\n",
      "4             aquino        72.33\n",
      "5             arroyo        72.15\n",
      "6              ninoy        71.73\n",
      "7             imelda        69.82\n",
      "8            corazon        68.44\n",
      "9            napoles        68.20\n",
      "10  macapagal-arroyo        66.66\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "         Word  %Similarity\n",
      "1   ferdinand        74.80\n",
      "2      imelda        69.06\n",
      "3       marco        65.94\n",
      "4      aquino        61.34\n",
      "5   cojuangco        60.59\n",
      "6       lucas        58.97\n",
      "7       mateo        58.70\n",
      "8     corazon        57.94\n",
      "9       ponce        57.09\n",
      "10    elpidio        54.48\n"
     ]
    }
   ],
   "source": [
    "simGen(\"marcos\") #pangngalang pantangi, proper noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "       Word  %Similarity\n",
      "1      tupa        83.24\n",
      "2       aso        82.38\n",
      "3       paa        78.82\n",
      "4      ahas        78.50\n",
      "5    puting        78.24\n",
      "6     buhok        78.03\n",
      "7   kambing        77.28\n",
      "8      ibon        77.23\n",
      "9     itlog        76.46\n",
      "10   sungay        76.45\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "                Word  %Similarity\n",
      "1           kabayong        89.03\n",
      "2   kabayo-kabayohan        80.29\n",
      "3               tupa        67.52\n",
      "4            kambing        67.27\n",
      "5            kahugis        62.02\n",
      "6          nakasakay        61.68\n",
      "7            palayok        60.71\n",
      "8             sungay        60.65\n",
      "9             odiseo        60.54\n",
      "10               aso        59.68\n"
     ]
    }
   ],
   "source": [
    "simGen(\"kabayo\") #pangngalan pambalana, common noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "     Word  %Similarity\n",
      "1      ka        86.80\n",
      "2    ikaw        86.36\n",
      "3    kami        85.93\n",
      "4    kayo        85.73\n",
      "5    inyo        84.79\n",
      "6      po        81.98\n",
      "7    akin        81.53\n",
      "8    tayo        81.27\n",
      "9     iyo        81.17\n",
      "10  ninyo        79.98\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "      Word  %Similarity\n",
      "1    ako'y        81.86\n",
      "2       ko        78.61\n",
      "3    akong        78.39\n",
      "4     akin        74.45\n",
      "5    aking        72.63\n",
      "6     ikaw        72.16\n",
      "7     kayo        71.68\n",
      "8       po        69.39\n",
      "9     inyo        67.48\n",
      "10  siguro        66.40\n"
     ]
    }
   ],
   "source": [
    "simGen(\"ako\") #panghalip, pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "           Word  %Similarity\n",
      "1          sina        70.96\n",
      "2            ni        68.66\n",
      "3          kina        68.09\n",
      "4   mag-asawang        60.55\n",
      "5       michael        58.07\n",
      "6          john        57.86\n",
      "7        martin        55.89\n",
      "8          leon        55.57\n",
      "9        albert        55.02\n",
      "10       joseph        54.94\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "              Word  %Similarity\n",
      "1             sina        80.13\n",
      "2               ni        74.22\n",
      "3   pinagbibidahan        68.50\n",
      "4             kina        65.60\n",
      "5     pinagbidahan        65.39\n",
      "6            lloyd        64.20\n",
      "7          rogelio        63.90\n",
      "8            eddie        63.63\n",
      "9            edgar        63.43\n",
      "10              si        63.09\n"
     ]
    }
   ],
   "source": [
    "simGen(\"nina\") #pang-ukol, preposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "          Word  %Similarity\n",
      "1       mabuti        80.89\n",
      "2       pangit        79.69\n",
      "3       masaya        78.59\n",
      "4       madali        78.03\n",
      "5   interesado        77.31\n",
      "6     marunong        75.45\n",
      "7        gusto        73.42\n",
      "8         akin        73.00\n",
      "9      mahirap        72.58\n",
      "10      masama        72.56\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "           Word  %Similarity\n",
      "1     magandang        85.59\n",
      "2   magagandang        68.25\n",
      "3         ganda        67.66\n",
      "4         akala        59.86\n",
      "5     masyadong        59.43\n",
      "6        mabait        59.27\n",
      "7        madali        58.88\n",
      "8       mahilig        58.75\n",
      "9        mabuti        58.56\n",
      "10       ganoon        58.22\n"
     ]
    }
   ],
   "source": [
    "simGen(\"maganda\") #pang-uri, adjective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Two: Incomplete word analogy (5 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(worda, wordb, wordc):\n",
    "    wv_analogy = wv_model.wv.most_similar(negative = [worda], positive = [wordb, wordc])\n",
    "    wv_analogyDF = pd.DataFrame(wv_analogy,columns=[\"Word\",\"Similarity %\"]).rename(index = lambda x: x+1)\n",
    "    wv_analogyDF['Similarity %'] = round(wv_analogyDF['Similarity %'] * 100, 2)\n",
    "    print(f\"Word2Vec Result: \\n{wv_analogyDF}\\n\")\n",
    "    ft_analogy = ft_model.wv.most_similar(negative = [worda], positive = [wordb, wordc])\n",
    "    ft_analogyDF = pd.DataFrame(ft_analogy,columns=[\"Word\",\"Similarity %\"]).rename(index = lambda x: x+1)\n",
    "    ft_analogyDF['Similarity %'] = round(ft_analogyDF['Similarity %'] * 100, 2)\n",
    "    print(f\"FastText Result: \\n{ft_analogyDF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case word for word2vec is OOV but present in fasttext\n",
    "def analogy2(worda, wordb, wordc):\n",
    "#     wv_analogy = wv_model.wv.most_similar(negative = [worda], positive = [wordb, wordc])\n",
    "#     wv_analogyDF = pd.DataFrame(wv_analogy,columns=[\"Word\",\"Similarity %\"]).rename(index = lambda x: x+1)\n",
    "#     wv_analogyDF['Similarity %'] = round(wv_analogyDF['Similarity %'] * 100, 2)\n",
    "#     print(f\"Word2Vec Result: \\n{wv_analogyDF}\\n\")\n",
    "    ft_analogy = ft_model.wv.most_similar(negative = [worda], positive = [wordb, wordc])\n",
    "    ft_analogyDF = pd.DataFrame(ft_analogy,columns=[\"Word\",\"Similarity %\"]).rename(index = lambda x: x+1)\n",
    "    ft_analogyDF['Similarity %'] = round(ft_analogyDF['Similarity %'] * 100, 2)\n",
    "    print(f\"FastText Result: \\n{ft_analogyDF}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Result: \n",
      "           Word  Similarity %\n",
      "1         kahel         72.89\n",
      "2          asul         67.86\n",
      "3         dilaw         66.26\n",
      "4         rosas         64.64\n",
      "5          puti         60.20\n",
      "6        saging         60.10\n",
      "7          lila         60.04\n",
      "8   pinaghalong         59.44\n",
      "9     tsokolate         59.32\n",
      "10        lunti         59.26\n",
      "\n",
      "FastText Result: \n",
      "          Word  Similarity %\n",
      "1        kulay         62.92\n",
      "2        berde         59.32\n",
      "3         lila         58.92\n",
      "4        dilaw         56.73\n",
      "5         asul         54.65\n",
      "6         puti         54.23\n",
      "7   kayumanggi         53.34\n",
      "8          emu         51.53\n",
      "9        lunti         51.05\n",
      "10        pula         50.58\n"
     ]
    }
   ],
   "source": [
    "analogy(\"aklat\", \"libro\", \"bughaw\") #Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Result: \n",
      "        Word  Similarity %\n",
      "1     butong         72.65\n",
      "2       usok         71.39\n",
      "3      baboy         70.34\n",
      "4       isda         69.61\n",
      "5      katas         69.21\n",
      "6    kambing         68.82\n",
      "7      karne         68.40\n",
      "8     tuyong         68.34\n",
      "9       buto         67.92\n",
      "10  alikabok         67.81\n",
      "\n",
      "FastText Result: \n",
      "       Word  Similarity %\n",
      "1    munggo         56.47\n",
      "2     dagat         56.05\n",
      "3     hipon         55.81\n",
      "4     gatas         54.82\n",
      "5     bayag         54.54\n",
      "6      suka         54.25\n",
      "7    harina         53.31\n",
      "8   karneng         51.95\n",
      "9   bungang         51.42\n",
      "10     suso         51.33\n"
     ]
    }
   ],
   "source": [
    "#analogy(\"isda\",\"ilog\",\"ibon\") #related words\n",
    "analogy(\"kotse\",\"eroplano\",\"itlog\") #Similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Result: \n",
      "          Word  Similarity %\n",
      "1     sasakyan         66.41\n",
      "2       puwang         59.23\n",
      "3        tubig         58.26\n",
      "4     pagawaan         58.03\n",
      "5         tubo         57.35\n",
      "6   malalaking         56.90\n",
      "7          bag         56.49\n",
      "8       tubong         56.34\n",
      "9       pakpak         56.13\n",
      "10        yelo         56.08\n",
      "\n",
      "FastText Result: \n",
      "           Word  Similarity %\n",
      "1   unti-unting         49.05\n",
      "2         lubid         48.86\n",
      "3      pagawaan         46.06\n",
      "4          gulo         44.82\n",
      "5     unti-unti         44.59\n",
      "6          yaon         44.41\n",
      "7    kagamitang         44.34\n",
      "8    bakteryang         43.97\n",
      "9          yari         43.83\n",
      "10        uling         43.63\n"
     ]
    }
   ],
   "source": [
    "analogy(\"pinto\",\"bahay\",\"gulong\") #part-whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Result: \n",
      "          Word  Similarity %\n",
      "1      pagdaan         53.36\n",
      "2     taglagas         49.67\n",
      "3      magmula         45.54\n",
      "4     tag-araw         44.20\n",
      "5    pagkaraan         44.18\n",
      "6     taglamig         44.05\n",
      "7   pagkaraang         44.02\n",
      "8     tagsibol         43.36\n",
      "9     tag-ulan         43.35\n",
      "10        yelo         42.70\n",
      "\n",
      "FastText Result: \n",
      "          Word  Similarity %\n",
      "1      sumapit         56.99\n",
      "2      matapos         48.81\n",
      "3    pagkaraan         48.81\n",
      "4     pagsapit         48.38\n",
      "5   pagkatapos         48.05\n",
      "6    magsimula         47.02\n",
      "7   hatinggabi         46.39\n",
      "8       kailan         46.30\n",
      "9   pagkaraang         46.27\n",
      "10        muli         46.00\n"
     ]
    }
   ],
   "source": [
    "analogy(\"maliwanag\",\"madilim\",\"bago\") #antonmys, expecting for luma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Result: \n",
      "            Word  Similarity %\n",
      "1           lawa         74.23\n",
      "2           ilog         73.57\n",
      "3          burol         71.84\n",
      "4   dalampasigan         71.46\n",
      "5      katubigan         70.37\n",
      "6          gubat         70.00\n",
      "7         lambak         69.46\n",
      "8     kabundukan         69.45\n",
      "9      dumadaloy         69.34\n",
      "10        hardin         69.16\n",
      "\n",
      "FastText Result: \n",
      "         Word  Similarity %\n",
      "1      lupang         63.88\n",
      "2      lupain         62.74\n",
      "3     lupaing         62.15\n",
      "4   katubigan         60.01\n",
      "5       dagat         59.42\n",
      "6       gubat         56.74\n",
      "7        ilog         56.62\n",
      "8   kagubatan         56.47\n",
      "9    bunganga         56.42\n",
      "10  karagatan         56.23\n"
     ]
    }
   ],
   "source": [
    "analogy(\"kotse\",\"lupa\",\"bangka\") #related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra (I did the manual cosine similarity of \"maganda\", and \"mabuti\")\n",
    "import numpy as np\n",
    "\n",
    "w1_vec = wv_model.wv.get_vector(\"lalaki\")\n",
    "w2_vec = wv_model.wv.get_vector(\"hari\")\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "    return vector1 @ vector2\n",
    "\n",
    "def magnitude(word_vector): #solver for the magnitude of word vector\n",
    "    return np.sqrt(np.sum(np.square(word_vector)))\n",
    "\n",
    "def cos_sim(dot_prod, mag1, mag2):\n",
    "    return (dot_prod) / (mag1* mag2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product (numerator): 48.79785919189453\n",
      "Magnitudes (denominators): 11.26809310913086, 10.900785446166992\n",
      "0.39727622\n"
     ]
    }
   ],
   "source": [
    "numerator = dot_product(w1_vec, w2_vec)\n",
    "magnitude1 = magnitude(w1_vec)\n",
    "magnitude2 = magnitude(w2_vec)\n",
    "cosine_sim = cos_sim(numerator, magnitude1, magnitude2)\n",
    "print(f\"Dot product (numerator): {numerator}\")\n",
    "print(f\"Magnitudes (denominators): {magnitude1}, {magnitude2}\")\n",
    "print(cosine_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('babae', 0.7392164468765259),\n",
       " ('babaeng', 0.707935094833374),\n",
       " ('batang', 0.6850568652153015),\n",
       " ('lalaking', 0.6792464256286621),\n",
       " ('ina', 0.6285486817359924),\n",
       " ('kapatid', 0.6119673252105713),\n",
       " ('nakatatandang', 0.6075147390365601),\n",
       " ('aktor', 0.6047272682189941),\n",
       " ('lalake', 0.598362386226654),\n",
       " ('pilipina', 0.5968809127807617)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hari:lalaki :: reyna:?\n",
    "#hari:lalaki :: reyna:? = babae\n",
    "wv_model.wv.most_similar(positive = ['reyna','lalaki'], negative = [\"hari\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "w1_vec = wv_model.wv.get_vector(\"hari\")\n",
    "w2_vec = wv_model.wv.get_vector(\"lalaki\")\n",
    "w3_vec = wv_model.wv.get_vector(\"reyna\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.980848  , -1.0779285 ,  0.09271973,  0.8678707 , -1.9782072 ,\n",
       "        1.7537644 , -0.7765211 , -2.136334  ,  0.30188447, -0.28930473,\n",
       "        0.68389446, -1.4410336 ,  0.8796174 ,  2.2108216 ,  2.4252014 ,\n",
       "       -1.0783155 ,  2.418281  , -1.2244966 ,  0.34204695,  2.1893706 ,\n",
       "       -0.66234577, -2.526125  , -0.47509235, -0.2607782 , -2.3952792 ,\n",
       "        1.2119874 , -0.5471051 , -0.40477008,  0.86579525,  2.676379  ,\n",
       "       -0.6534424 , -1.4619862 ,  0.80614924,  0.60152006,  0.45867538,\n",
       "        0.8882272 , -2.3444588 , -0.5608117 ,  0.28402844, -2.0691013 ,\n",
       "       -0.28398585,  1.4977462 , -1.4276516 , -1.5042715 , -0.77241933,\n",
       "        1.4725615 ,  1.5743334 ,  0.47296566,  0.04112101,  0.43304265,\n",
       "       -1.6315625 ,  0.08503836, -0.8721514 ,  0.78358126,  1.581552  ,\n",
       "       -2.061219  ,  1.3549744 , -0.7528177 , -2.2347038 ,  0.83104604,\n",
       "       -0.25596216, -1.4678438 , -0.7331305 , -0.16097316,  0.7010161 ,\n",
       "       -0.85828084, -0.37374306,  2.9011388 ,  0.9919684 , -0.2840498 ,\n",
       "        0.7401963 , -0.40388697,  1.0865097 , -0.3780217 ,  1.3681927 ,\n",
       "        0.65111566,  0.13447885,  0.01620224, -2.3029685 , -0.64401484,\n",
       "       -0.92251694,  0.70359206,  0.46830064, -0.7494815 , -1.1167228 ,\n",
       "       -1.1977439 , -0.8397939 ,  1.1794984 ,  0.01087422, -1.746084  ,\n",
       "       -1.4172264 , -1.3562765 ,  0.5577493 , -1.6724285 ,  0.15558136,\n",
       "        2.6232533 ,  1.3774316 , -1.0697366 ,  1.7140007 , -0.5912959 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = (w3_vec + w2_vec) - w1_vec\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lalaki', 0.7826639413833618)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_model.wv.most_similar(positive = [vec], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Day 08/31/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "          Word  %Similarity\n",
      "1       babang        68.49\n",
      "2       sakong        67.70\n",
      "3         kuko        67.39\n",
      "4        tenga        66.49\n",
      "5        lapad        66.17\n",
      "6         lila        65.52\n",
      "7        biyas        65.05\n",
      "8      balikat        64.59\n",
      "9   sentimetro        64.37\n",
      "10      kaliwa        64.34\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "          Word  %Similarity\n",
      "1       bababa        64.05\n",
      "2        ibaba        58.45\n",
      "3       babang        57.01\n",
      "4         taas        56.32\n",
      "5        itaas        54.62\n",
      "6      baywang        53.58\n",
      "7     balakang        52.69\n",
      "8   sentimetro        50.99\n",
      "9          noo        50.85\n",
      "10      kanang        49.81\n"
     ]
    }
   ],
   "source": [
    "#word1\n",
    "simGen(\"baba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "           Word  %Similarity\n",
      "1         hinog        68.32\n",
      "2      matapang        67.41\n",
      "3          lila        66.64\n",
      "4   transparent        66.12\n",
      "5          mura        66.09\n",
      "6      niluluto        65.81\n",
      "7         patak        64.57\n",
      "8           nil        64.38\n",
      "9          tuyo        64.36\n",
      "10      replace        64.33\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "        Word  %Similarity\n",
      "1     mabasa        64.40\n",
      "2     nabasa        60.13\n",
      "3   binabasa        52.55\n",
      "4   mababasa        51.32\n",
      "5      basal        49.75\n",
      "6    pagbasa        48.84\n",
      "7     binasa        48.70\n",
      "8      amino        47.64\n",
      "9   buhangin        46.85\n",
      "10   marinig        46.27\n"
     ]
    }
   ],
   "source": [
    "#word2\n",
    "simGen(\"basa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "           Word  %Similarity\n",
      "1        lalaki        92.49\n",
      "2        lalake        81.75\n",
      "3       babaeng        77.86\n",
      "4      lalaking        75.26\n",
      "5        batang        72.59\n",
      "6          anak        72.32\n",
      "7        dalaga        68.58\n",
      "8       kapatid        67.60\n",
      "9       sanggol        66.68\n",
      "10  kalalakihan        65.58\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "           Word  %Similarity\n",
      "1        lalaki        89.24\n",
      "2       babaeng        81.45\n",
      "3        lalake        79.75\n",
      "4      lalaking        78.30\n",
      "5   kalalakihan        71.21\n",
      "6    kababaihan        68.79\n",
      "7      lalakeng        67.87\n",
      "8      pambabae        66.12\n",
      "9        batang        66.03\n",
      "10   pambabaeng        65.84\n"
     ]
    }
   ],
   "source": [
    "#word3\n",
    "simGen(\"babae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "     Word  %Similarity\n",
      "1      ka        86.80\n",
      "2    ikaw        86.36\n",
      "3    kami        85.93\n",
      "4    kayo        85.73\n",
      "5    inyo        84.79\n",
      "6      po        81.98\n",
      "7    akin        81.53\n",
      "8    tayo        81.27\n",
      "9     iyo        81.17\n",
      "10  ninyo        79.98\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "      Word  %Similarity\n",
      "1    ako'y        81.86\n",
      "2       ko        78.61\n",
      "3    akong        78.39\n",
      "4     akin        74.45\n",
      "5    aking        72.63\n",
      "6     ikaw        72.16\n",
      "7     kayo        71.68\n",
      "8       po        69.39\n",
      "9     inyo        67.48\n",
      "10  siguro        66.40\n"
     ]
    }
   ],
   "source": [
    "#word4\n",
    "simGen(\"ako\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "           Word  %Similarity\n",
      "1       hayskul        68.09\n",
      "2      kolehiyo        67.80\n",
      "3   elementarya        67.32\n",
      "4       tahanan        66.64\n",
      "5    bilangguan        63.86\n",
      "6         bahay        63.77\n",
      "7      kumbento        63.14\n",
      "8      paaralan        62.64\n",
      "9       bakuran        61.91\n",
      "10     kulungan        61.70\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "            Word  %Similarity\n",
      "1       hospital        86.61\n",
      "2         duktor        61.34\n",
      "3        tahanan        60.40\n",
      "4           nars        59.82\n",
      "5        medical        59.49\n",
      "6     sementeryo        58.07\n",
      "7        medikal        57.75\n",
      "8       siruhiya        57.20\n",
      "9      seminaryo        57.05\n",
      "10  pagtatrabaho        56.55\n"
     ]
    }
   ],
   "source": [
    "#word5\n",
    "simGen(\"ospital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "             Word  %Similarity\n",
      "1        illinois        85.53\n",
      "2        michigan        84.27\n",
      "3            dame        83.13\n",
      "4        memorial        82.67\n",
      "5       municipal        82.58\n",
      "6         general        81.64\n",
      "7           beach        81.47\n",
      "8         indiana        81.43\n",
      "9          police        81.42\n",
      "10  massachusetts        81.31\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "            Word  %Similarity\n",
      "1        ospital        86.61\n",
      "2        medical        71.76\n",
      "3     children's        62.05\n",
      "4       children        60.96\n",
      "5         school        60.08\n",
      "6      princeton        59.21\n",
      "7       training        58.79\n",
      "8           nars        57.96\n",
      "9   headquarters        57.41\n",
      "10     seminaryo        57.33\n"
     ]
    }
   ],
   "source": [
    "#word6\n",
    "simGen(\"hospital\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Marcos' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-b679b49c2f1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#word7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Marcos\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Marcos will return an out of vocabulary error as all of the words in my pretrained word embeddings are encoded in lower case, but \"marcos\" will run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8f9d6d02e800>\u001b[0m in \u001b[0;36msimGen\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mft_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwvDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%Similarity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwvDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%Similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwvDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%Similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Marcos' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#word7\n",
    "simGen(\"Marcos\") #Marcos will return an out of vocabulary error as all of the words in my pretrained word embeddings are encoded in lower case, but \"marcos\" will run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "                Word  %Similarity\n",
      "1          pangulong        75.88\n",
      "2          ferdinand        72.92\n",
      "3            estrada        72.53\n",
      "4             aquino        72.33\n",
      "5             arroyo        72.15\n",
      "6              ninoy        71.73\n",
      "7             imelda        69.82\n",
      "8            corazon        68.44\n",
      "9            napoles        68.20\n",
      "10  macapagal-arroyo        66.66\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "         Word  %Similarity\n",
      "1   ferdinand        74.80\n",
      "2      imelda        69.06\n",
      "3       marco        65.94\n",
      "4      aquino        61.34\n",
      "5   cojuangco        60.59\n",
      "6       lucas        58.97\n",
      "7       mateo        58.70\n",
      "8     corazon        57.94\n",
      "9       ponce        57.09\n",
      "10    elpidio        54.48\n"
     ]
    }
   ],
   "source": [
    "#Rerun of word7 in lower case\n",
    "simGen(\"marcos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Piolo' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-23aa6e56e387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#word8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msimGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Piolo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8f9d6d02e800>\u001b[0m in \u001b[0;36msimGen\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mft_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwvDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%Similarity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwvDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%Similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwvDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%Similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Piolo' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#word8\n",
    "simGen(\"Piolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'piolo' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-cba7dabe2b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"piolo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-8f9d6d02e800>\u001b[0m in \u001b[0;36msimGen\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msimGen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mft_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwvDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%Similarity\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwvDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%Similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwvDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%Similarity'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'piolo' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "#rerun word8 in lowercase\n",
    "simGen(\"piolo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "          Word  %Similarity\n",
      "1       sabado        87.19\n",
      "2         gabi        87.09\n",
      "3        lunes        86.05\n",
      "4   hatinggabi        84.01\n",
      "5     biyernes        82.70\n",
      "6   miyerkules        82.63\n",
      "7      huwebes        81.41\n",
      "8       martes        79.51\n",
      "9        alas-        78.77\n",
      "10    tanghali        78.25\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "          Word  %Similarity\n",
      "1   hatinggabi        79.18\n",
      "2        alas-        75.97\n",
      "3         gabi        73.25\n",
      "4       sabado        69.23\n",
      "5         alas        67.20\n",
      "6     tanghali        65.73\n",
      "7      huwebes        65.07\n",
      "8   miyerkules        64.87\n",
      "9     biyernes        64.15\n",
      "10       lunes        61.62\n"
     ]
    }
   ],
   "source": [
    "#word9\n",
    "simGen(\"umaga\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Results: \n",
      "         Word  %Similarity\n",
      "1       bigas        90.87\n",
      "2      harina        89.76\n",
      "3      saging        89.71\n",
      "4        suka        88.75\n",
      "5   pampalasa        88.61\n",
      "6       karne        88.39\n",
      "7        tsaa        88.17\n",
      "8       niyog        88.16\n",
      "9      asukal        87.98\n",
      "10      gulay        87.97\n",
      "\n",
      "\n",
      "FastText Results: \n",
      "        Word  %Similarity\n",
      "1      gulay        76.32\n",
      "2      niyog        75.54\n",
      "3      sarsa        73.35\n",
      "4    sibuyas        73.31\n",
      "5       mais        73.30\n",
      "6    patatas        72.19\n",
      "7   niluluto        72.07\n",
      "8      sahog        71.18\n",
      "9      bigas        70.24\n",
      "10   mantika        69.51\n"
     ]
    }
   ],
   "source": [
    "#word10\n",
    "simGen(\"kape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'breakfast' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a5a6d6668379>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"umaga\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"breakfast\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"gabi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-7780d10ceadf>\u001b[0m in \u001b[0;36manalogy\u001b[0;34m(worda, wordb, wordc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwv_analogy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mworda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwordb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mwv_analogyDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_analogy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Similarity %\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwv_analogyDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Similarity %'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_analogyDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Similarity %'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Word2Vec Result: \\n{wv_analogyDF}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'breakfast' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "analogy(\"umaga\",\"breakfast\",\"gabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText Result: \n",
      "                   Word  Similarity %\n",
      "1              breaking         70.73\n",
      "2                 break         67.73\n",
      "3          breakingnews         63.46\n",
      "4                  last         61.55\n",
      "5                 years         60.59\n",
      "6   breaking-news-world         59.70\n",
      "7           competition         58.35\n",
      "8               world's         58.31\n",
      "9                 can't         57.84\n",
      "10           everything         57.73\n"
     ]
    }
   ],
   "source": [
    "analogy2(\"umaga\",\"breakfast\",\"gabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Result: \n",
      "           Word  Similarity %\n",
      "1         nanay         82.68\n",
      "2          lolo         81.47\n",
      "3    kasintahan         80.30\n",
      "4           ate         79.66\n",
      "5     minamahal         77.94\n",
      "6   pinakasalan         76.63\n",
      "7     kaibigang         76.62\n",
      "8       tiyuhin         76.52\n",
      "9          lola         76.02\n",
      "10         kuya         73.51\n",
      "\n",
      "FastText Result: \n",
      "          Word  Similarity %\n",
      "1        nanay         67.15\n",
      "2   kasintahan         64.54\n",
      "3         lolo         62.79\n",
      "4         lola         62.12\n",
      "5         tita         61.75\n",
      "6       pinsan         61.36\n",
      "7          ina         60.66\n",
      "8      asawang         58.64\n",
      "9      mercado         58.57\n",
      "10   velasquez         58.04\n"
     ]
    }
   ],
   "source": [
    "analogy(\"lalaki\",\"tatay\",\"babae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Result: \n",
      "      Word  Similarity %\n",
      "1    balat         72.90\n",
      "2   pakpak         70.78\n",
      "3    sugat         69.65\n",
      "4    buhok         68.86\n",
      "5     mata         67.82\n",
      "6    laman         67.75\n",
      "7    lason         66.77\n",
      "8     buto         66.42\n",
      "9     amoy         65.81\n",
      "10    titi         65.35\n",
      "\n",
      "FastText Result: \n",
      "            Word  Similarity %\n",
      "1     nagdudulot         53.75\n",
      "2     nagbubunga         53.43\n",
      "3          tenga         52.84\n",
      "4          bunga         52.80\n",
      "5        mukhang         50.83\n",
      "6        bubuyog         50.29\n",
      "7            uod         50.14\n",
      "8       alimango         49.80\n",
      "9   nagreresulta         49.78\n",
      "10          leeg         49.65\n"
     ]
    }
   ],
   "source": [
    "analogy(\"ospital\",\"sakit\",\"bahay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'kain' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-f9c391a0979e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manalogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kape\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mainit\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"kain\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-54-7780d10ceadf>\u001b[0m in \u001b[0;36manalogy\u001b[0;34m(worda, wordb, wordc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0manalogy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mwv_analogy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mworda\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mwordb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwordc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mwv_analogyDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_analogy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Word\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Similarity %\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mwv_analogyDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Similarity %'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv_analogyDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Similarity %'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Word2Vec Result: \\n{wv_analogyDF}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'kain' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "analogy(\"kape\",\"mainit\",\"kain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText Result: \n",
      "        Word  Similarity %\n",
      "1   sariwang         60.22\n",
      "2     kainin         59.92\n",
      "3     kinain         57.40\n",
      "4   kumakain         57.15\n",
      "5   kinakain         55.32\n",
      "6   nakakain         55.10\n",
      "7     kumain         54.46\n",
      "8    maiinit         53.65\n",
      "9    pagkain         52.93\n",
      "10     hayop         52.36\n"
     ]
    }
   ],
   "source": [
    "analogy2(\"kape\",\"mainit\",\"kain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Result: \n",
      "              Word  Similarity %\n",
      "1            buwan         74.42\n",
      "2           linggo         73.89\n",
      "3             gabi         64.63\n",
      "4             oras         64.53\n",
      "5           buwang         63.58\n",
      "6      pagdiriwang         62.50\n",
      "7            umaga         59.53\n",
      "8             taon         58.43\n",
      "9        kaganapan         57.76\n",
      "10  ipinagdiriwang         56.53\n",
      "\n",
      "FastText Result: \n",
      "            Word  Similarity %\n",
      "1       bisyesto         57.02\n",
      "2       bisperas         56.14\n",
      "3      paggunita         54.02\n",
      "4     pagkabuhay         52.63\n",
      "5          buwan         52.03\n",
      "6   kalendaryong         51.17\n",
      "7          pasko         50.41\n",
      "8       kaarawan         50.32\n",
      "9          umaga         49.60\n",
      "10       eklipse         49.52\n"
     ]
    }
   ],
   "source": [
    "analogy(\"ulan\",\"bagyo\",\"araw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
